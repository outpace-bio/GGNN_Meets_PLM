{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn.metrics as sk_metrics\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "# import tqdm\n",
    "from atom3d.datasets import LMDBDataset\n",
    "from atom3d.splits.splits import split_randomly\n",
    "from atom3d.util import metrics\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import gvp\n",
    "import gvp.atom3d\n",
    "from gvp import set_seed, Logger\n",
    "from egnn import egnn_clean as eg\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "\n",
    "# import atom3d.datasets.datasets as da\n",
    "# dataset = da.load_dataset('../data/PPI/DIPS-split/data/', 'pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-24 04:38:03,435 INFO 144587: Splitting dataset with 30472 entries.\n",
      "2023-02-24 04:38:03,436 INFO 144587: Size of the training set: 24378\n",
      "2023-02-24 04:38:03,436 INFO 144587: Size of the validation set: 3047\n",
      "2023-02-24 04:38:03,437 INFO 144587: Size of the test set: 3047\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "data_path = '../data/paul_PPbind/test/'\n",
    "# dataset = LMDBDataset(data_path + 'train', transform=gvp.atom3d.PPBindingTransform(plm=0, device='cpu'))\n",
    "dataset = LMDBDataset(data_path, transform=gvp.atom3d.PPBindingTransform(plm=1, device=device))\n",
    "trainset, valset, testset = split_randomly(dataset)\n",
    "\n",
    "dl = torch_geometric.loader.DataLoader(dataset=testset,\n",
    "                                            num_workers=0,\n",
    "                                            batch_size=8,\n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = eg.PPBindingModel(plm=1, device=device)\n",
    "d = torch.load('../models/PPBind_centered_coords_egnn_plm1_epoch3_val_loss_0.14324405546033672.chkpt', map_location=device)\n",
    "d = {k.replace('module.', ''): v for k, v in d.items()}\n",
    "model.load_state_dict(d)\n",
    "model = model.eval().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>labels</th>\n",
       "      <th>call</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.751482e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.499002e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.527373e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.431825e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.555949e-02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>3.974957e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>8.633540e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>9.359431e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>9.771439e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>3.348189e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>808 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            preds  labels  call\n",
       "0    9.751482e-01       1     1\n",
       "1    9.499002e-01       1     1\n",
       "2    9.527373e-01       1     1\n",
       "3    9.431825e-01       1     1\n",
       "4    1.555949e-02       0     0\n",
       "..            ...     ...   ...\n",
       "803  3.974957e-06       0     0\n",
       "804  8.633540e-01       1     1\n",
       "805  9.359431e-01       1     1\n",
       "806  9.771439e-01       1     1\n",
       "807  3.348189e-08       0     0\n",
       "\n",
       "[808 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dl))\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "for bnum, batch in enumerate(dl):\n",
    "    preds.extend(list(torch.sigmoid(model(batch)).detach().cpu().numpy()))\n",
    "    labels.extend(list(batch[0].label.detach().cpu().numpy()))\n",
    "\n",
    "    if bnum == 100:\n",
    "        break\n",
    "\n",
    "preds = pd.DataFrame({'preds': preds, 'labels': labels})\n",
    "preds['call'] = preds['preds'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.0540375e-07, -9.0638468e-07,  4.9654119e-07], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "p1 = batch[1]\n",
    "sample_idx = 0\n",
    "chain1 = p1.x[p1.ptr[sample_idx]:p1.ptr[sample_idx+1]].detach().cpu().numpy()\n",
    "chain1.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       393\n",
      "           1       0.95      0.95      0.95       415\n",
      "\n",
      "    accuracy                           0.95       808\n",
      "   macro avg       0.95      0.95      0.95       808\n",
      "weighted avg       0.95      0.95      0.95       808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summarize the fit of the model, print the confusion matrix\n",
    "\n",
    "print(classification_report(preds['labels'], preds['call']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>labels</th>\n",
       "      <th>call</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.751482e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.499002e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.527373e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.431825e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.555949e-02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>3.974957e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>8.633540e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>9.359431e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>9.771439e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>3.348189e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>808 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            preds  labels  call\n",
       "0    9.751482e-01       1     1\n",
       "1    9.499002e-01       1     1\n",
       "2    9.527373e-01       1     1\n",
       "3    9.431825e-01       1     1\n",
       "4    1.555949e-02       0     0\n",
       "..            ...     ...   ...\n",
       "803  3.974957e-06       0     0\n",
       "804  8.633540e-01       1     1\n",
       "805  9.359431e-01       1     1\n",
       "806  9.771439e-01       1     1\n",
       "807  3.348189e-08       0     0\n",
       "\n",
       "[808 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9492574257425742"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[preds['labels'] == preds['call']].shape[0] / preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.717462420463562\n",
      "10 0.6931776583194733\n",
      "20 0.69017853140831\n",
      "30 0.6965782046318054\n",
      "40 0.6903312802314758\n",
      "50 0.6791554868221283\n",
      "60 0.6595579624176026\n",
      "70 0.6180838644504547\n",
      "80 0.6023252129554748\n",
      "90 0.5775288462638855\n",
      "100 0.587931090593338\n",
      "110 0.5414257317781448\n",
      "120 0.4844168394804001\n",
      "130 0.5293464481830596\n",
      "140 0.5114886581897735\n",
      "150 0.46706070601940153\n",
      "160 0.43244494795799254\n",
      "170 0.4818073779344559\n",
      "180 0.5525013148784638\n",
      "190 0.4985406190156937\n",
      "200 0.4412555664777756\n",
      "210 0.4781623274087906\n",
      "220 0.4804361343383789\n",
      "230 0.415347820520401\n",
      "240 0.37652755379676817\n",
      "250 0.3916472226381302\n",
      "260 0.4876827001571655\n",
      "270 0.44699072241783144\n",
      "280 0.42157927006483076\n",
      "290 0.45795753300189973\n",
      "300 0.40810640454292296\n",
      "310 0.3996733009815216\n",
      "320 0.42279467433691026\n",
      "330 0.4264607042074203\n",
      "340 0.4135534703731537\n",
      "350 0.41993205845355985\n",
      "360 0.4567837119102478\n",
      "370 0.37821575701236726\n",
      "380 0.4016067236661911\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.6, patience=5, min_lr=5e-7)\n",
    "losses = []\n",
    "for bnum, batch in enumerate(dl):\n",
    "    out = model(batch)\n",
    "    labels = batch[0].label.float()\n",
    "\n",
    "    # Calculate loss for binary labels\n",
    "    loss = nn.BCEWithLogitsLoss()(out, labels)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    # for i in range(len(batch)):\n",
    "    #     out.append(model(batch[i]))\n",
    "\n",
    "    if bnum % 10 == 0:\n",
    "        print(bnum, np.mean(losses[-10:]))\n",
    "    if bnum == 1000:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7317, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast out as torch float\n",
    "\n",
    "\n",
    "out.squeeze().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5231],\n",
       "        [0.5392],\n",
       "        [0.4919]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataBatch(x=[937, 3], edge_index=[2, 9072], atoms=[937], edge_s=[9072, 16], edge_v=[9072, 1, 3], label=[3], batch=[937], ptr=[4]),\n",
       " DataBatch(x=[836, 3], edge_index=[2, 8454], atoms=[836], edge_s=[8454, 16], edge_v=[8454, 1, 3], label=[3], batch=[836], ptr=[4])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataBatch(x=[669, 3], edge_index=[2, 6368], atoms=[669], edge_s=[6368, 16], edge_v=[6368, 1, 3], label=[669], batch=[669], ptr=[4]),\n",
       " DataBatch(x=[531, 3], edge_index=[2, 5080], atoms=[531], edge_s=[5080, 16], edge_v=[5080, 1, 3], label=[531], batch=[531], ptr=[4])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dl))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[378, 3], edge_index=[2, 3550], atoms=[378], edge_s=[3550, 16], edge_v=[3550, 1, 3], label=[378], batch=[378], ptr=[3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m t \u001b[39m=\u001b[39m trainset[\u001b[39m105\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m output \u001b[39m=\u001b[39m model(t)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      3\u001b[0m cutoff \u001b[39m=\u001b[39m \u001b[39m0.3\u001b[39m\n\u001b[1;32m      5\u001b[0m pred_1 \u001b[39m=\u001b[39m (output \u001b[39m>\u001b[39m cutoff)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)[:t[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m/opt/conda/envs/ggnn_plm/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/psample/GGNN_Meets_PLM/notebooks/../egnn/egnn_clean.py:269\u001b[0m, in \u001b[0;36mPPIModel.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    268\u001b[0m     graph1, graph2 \u001b[39m=\u001b[39m batch\n\u001b[0;32m--> 269\u001b[0m     out1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(\u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(graph1))\n\u001b[1;32m    270\u001b[0m     out2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(\u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mforward(graph2))\n\u001b[1;32m    271\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msigmoid(torch\u001b[39m.\u001b[39mcat([out1, out2])\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/envs/ggnn_plm/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/ggnn_plm/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/ggnn_plm/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/ggnn_plm/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "t = trainset[105]\n",
    "output = model(t).detach().numpy()\n",
    "cutoff = 0.3\n",
    "\n",
    "pred_1 = (output > cutoff).astype(int)[:t[0].x.shape[0]]\n",
    "pred_2 = (output > cutoff).astype(int)[t[0].x.shape[0]:]\n",
    "\n",
    "label_1 = t[0].label.numpy().astype(int)\n",
    "label_2 = t[1].label.numpy().astype(int)\n",
    "\n",
    "pred = pd.concat([pd.DataFrame({'pred': pred_1, 'label': label_1, 'protein': 1}),\n",
    "                 pd.DataFrame({'pred': pred_2, 'label': label_2, 'protein': 2})])\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(pred['label'], pred['pred'], average='binary')\n",
    "summary = pred.groupby(['label', 'pred'])['protein'].count().reset_index()\n",
    "summary['percent'] = summary['protein'] / summary.groupby('label')['protein'].transform('sum')\n",
    "\n",
    "print(f'Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}, Support: {support}')\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataBatch(x=[1512, 3], edge_index=[2, 14058], atoms=[1512], edge_s=[14058, 16], edge_v=[14058, 1, 3], label=[1512], plm=[1512, 1280], batch=[1512], ptr=[9]),\n",
       " DataBatch(x=[1433, 3], edge_index=[2, 13486], atoms=[1433], edge_s=[13486, 16], edge_v=[13486, 1, 3], label=[1433], plm=[1433, 1280], batch=[1433], ptr=[9])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trainset[110]\n",
    "y = trainset[2000]\n",
    "mix = (x[0], y[0])\n",
    "\n",
    "output = model(x).detach().numpy()\n",
    "cutoff = 0.3\n",
    "\n",
    "pred_1 = (output > cutoff).astype(int)[:x[0].x.shape[0]]\n",
    "pred_2 = (output > cutoff).astype(int)[x[0].x.shape[0]:]\n",
    "\n",
    "label_1 = x[0].label.numpy().astype(int)\n",
    "label_2 = x[1].label.numpy().astype(int)\n",
    "\n",
    "x = pd.concat([pd.DataFrame({'pred': pred_1, 'label': label_1, 'protein': 1}),\n",
    "                 pd.DataFrame({'pred': pred_2, 'label': label_2, 'protein': 2})])\n",
    "\n",
    "\n",
    "output = model(mix).detach().numpy()\n",
    "cutoff = 0.3\n",
    "\n",
    "pred_1 = (output > cutoff).astype(int)[:mix[0].x.shape[0]]\n",
    "pred_2 = (output > cutoff).astype(int)[mix[0].x.shape[0]:]\n",
    "\n",
    "label_1 = mix[0].label.numpy().astype(int)\n",
    "label_2 = mix[1].label.numpy().astype(int)\n",
    "\n",
    "mix = pd.concat([pd.DataFrame({'pred': pred_1, 'label': label_1, 'protein': 1}),\n",
    "                 pd.DataFrame({'pred': pred_2, 'label': label_2, 'protein': 2})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x[x['protein'] == 1]['pred'] == mix[mix['protein'] == 1]['pred']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn(module, input, output):\n",
    "    hook_fn.output = output\n",
    "\n",
    "model.dense[3].register_forward_hook(hook_fn)\n",
    "\n",
    "output = model(t)\n",
    "second_to_last_output = hook_fn.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_to_last_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 15, 19, 13, 19,  7,  5, 10, 16, 17, 11,  5, 18,  5,  0,  1, 19,  0,\n",
       "         0,  7,  3,  4, 19, 10, 12, 10, 14, 19,  7,  0, 10,  5,  6,  8,  7,  8,\n",
       "         8, 12,  4, 12,  2, 19,  3, 19, 10, 10, 14, 16,  0, 19,  4, 11,  1, 19,\n",
       "         0,  5,  1,  9,  7,  0, 10, 19, 12, 14,  7, 10,  6, 18,  7, 18, 11, 15,\n",
       "         6,  6, 11, 15,  7,  7,  7,  2,  8, 13, 14,  7, 16, 16, 15, 10,  3,  7,\n",
       "         0, 16, 10, 16,  7, 16, 19,  6,  3,  9,  9,  1,  5, 10,  0,  1,  8,  7,\n",
       "         0,  1,  1, 10, 19, 10, 12,  2,  7,  8, 18,  6,  2, 15, 12, 13,  9, 19,\n",
       "         5,  7,  9,  3, 10,  0, 10,  1,  5, 10,  1, 18,  0,  7,  9,  6,  3, 13,\n",
       "        11, 19, 19, 19, 10, 15, 18, 17,  3, 13, 19, 11,  3, 14,  0, 19,  9,  6,\n",
       "         6, 10, 18, 14,  5,  7, 13, 10,  7, 17,  3,  9,  5,  8,  7,  7, 19, 13,\n",
       "         5, 16, 15, 10, 12, 10,  0, 10, 18, 14,  3, 10, 19,  3, 10,  3,  1, 19,\n",
       "        19,  3,  8, 14, 14,  0, 16, 13, 14, 14, 18,  3, 19, 13, 14, 19,  3, 14,\n",
       "         0,  1, 16, 14,  0, 14,  7, 16, 10, 15, 15,  0, 11, 16,  0, 15,  1,  5,\n",
       "        11,  7,  5, 10,  9, 10,  5, 19,  4, 19,  6,  7,  9,  0,  3,  0,  9,  1,\n",
       "         5,  5, 13, 14, 14])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.9416e-26, 2.1925e-19, 8.3118e-14,  ..., 1.1505e-17, 6.0327e-24,\n",
       "         3.2499e-31],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.6126e-01, 6.2961e-01,\n",
       "         4.7287e-02],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.0241e-02, 7.6177e-01,\n",
       "         7.4300e-01],\n",
       "        ...,\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9982e-02, 5.2217e-01,\n",
       "         9.3435e-01],\n",
       "        [3.7651e-26, 1.4808e-19, 5.9831e-14,  ..., 1.6693e-17, 9.3289e-24,\n",
       "         5.3565e-31],\n",
       "        [0.0000e+00, 0.0000e+00, 1.4013e-45,  ..., 8.7249e-02, 9.9931e-04,\n",
       "         1.1759e-06]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t['edge_s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EGNN(\n",
       "  (embed): Embedding(22, 32)\n",
       "  (gcl_0): E_GCL(\n",
       "    (edge_mlp): Sequential(\n",
       "      (0): Linear(in_features=81, out_features=32, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (3): SiLU()\n",
       "    )\n",
       "    (node_mlp): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "    (coord_mlp): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=32, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (gcl_1): E_GCL(\n",
       "    (edge_mlp): Sequential(\n",
       "      (0): Linear(in_features=81, out_features=32, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (3): SiLU()\n",
       "    )\n",
       "    (node_mlp): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "    (coord_mlp): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=32, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (gcl_2): E_GCL(\n",
       "    (edge_mlp): Sequential(\n",
       "      (0): Linear(in_features=81, out_features=32, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (3): SiLU()\n",
       "    )\n",
       "    (node_mlp): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "    (coord_mlp): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=32, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (gcl_3): E_GCL(\n",
       "    (edge_mlp): Sequential(\n",
       "      (0): Linear(in_features=81, out_features=32, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (3): SiLU()\n",
       "    )\n",
       "    (node_mlp): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "    (coord_mlp): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=32, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101124"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "318**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(in_features=8, out_features=1, bias=True)\n",
    "x = torch.randn(10, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2156,  0.2200,  0.2242,  0.0592,  0.2251, -0.2823,  0.0648,  0.0478]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2128], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in layer.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4885, -0.6366,  0.1303, -0.2946,  0.9986,  0.2229,  0.1274,  0.6775],\n",
       "        [ 0.1450, -0.8104, -2.1237, -0.2748, -0.8929, -0.2312, -2.8565, -0.5362],\n",
       "        [ 0.9685,  0.1583, -0.3575,  1.1180,  0.0391,  1.5113, -1.5070, -0.7822],\n",
       "        [ 0.5667, -0.3549,  0.6392, -0.0496, -0.7520,  0.5488, -0.2220, -0.2424],\n",
       "        [ 0.1282,  0.6665, -0.3754,  0.9832, -1.1055,  1.5733, -0.4250,  0.5120],\n",
       "        [-0.1649, -1.4688, -0.9590,  0.7493, -2.1332, -0.8609, -1.3448,  0.6251],\n",
       "        [ 2.2488, -0.2345, -0.1216, -1.0074,  0.4047,  0.3989, -0.2835, -0.1022],\n",
       "        [-1.2178, -0.3435, -0.0983, -1.4357,  0.7948, -0.6118,  1.0365,  0.6706],\n",
       "        [ 0.6781, -0.0499, -1.5981,  0.6171,  1.2137,  0.5906, -1.4453, -1.8942],\n",
       "        [-0.4125,  0.2209, -0.4835,  0.5741,  2.2615, -0.8910,  1.7489,  0.4334]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x9 and 8x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m layer(x)\n",
      "File \u001b[0;32m/opt/conda/envs/ggnn_plm/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/ggnn_plm/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x9 and 8x1)"
     ]
    }
   ],
   "source": [
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ggnn_plm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f6ffb250edf756f7d7925b127d7ff21dc271f8adbd876afc9790f943cb23826"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
